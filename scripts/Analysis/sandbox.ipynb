{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.feature_extraction import create_feature_extractor, get_graph_node_names\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader\n",
    "#from utils.dataset import CustomImageDataset, CustomImageDataset_Adv\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/scratch/zsarwar/SparseDNNs/MT_cifar10_randCNN_10_8d64c1903fcb2ff6173fe22979c3175f/MT_Baseline_1f77b0596b5392db3af2aff2a02444c8/Checkpoints/model_best.pth.tar\"\n",
    "loc='cpu'\n",
    "x = torch.load(path, map_location=loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Kernel groups\n",
    "class ShardedKernels(nn.Module):\n",
    "    def __init__(self, num_quadrants):\n",
    "        super().__init__()\n",
    "        self.num_quadrants = num_quadrants\n",
    "\n",
    "    def generate_quad_coords(self, kernel_size):\n",
    "        step_size = max(1, kernel_size // 2)\n",
    "        cords_list = []\n",
    "        for r_start in range(0, kernel_size, step_size):\n",
    "            for c_start in range(0, kernel_size, step_size):\n",
    "                cords_list.append(((r_start, r_start + step_size), (c_start, c_start + step_size)))\n",
    "        return cords_list\n",
    "    \n",
    "    def forward(self, conv_filters):       \n",
    "        conv_filters_filtered = torch.ones(size=(conv_filters.shape[0], conv_filters.shape[1] // self.num_quadrants, conv_filters.shape[2], conv_filters.shape[3])).to(device=conv_filters.device)\n",
    "        quad_cords = self.generate_quad_coords(conv_filters.shape[-1])\n",
    "        kernel_depth_step = conv_filters.shape[1] // self.num_quadrants\n",
    "        k_depth_idx = 0\n",
    "        for r_cords, c_cords in quad_cords:\n",
    "            conv_filters_filtered[:, :, r_cords[0]:r_cords[1], c_cords[0]:c_cords[1]] = conv_filters[:, k_depth_idx:k_depth_idx + kernel_depth_step, r_cords[0]:r_cords[1], c_cords[0]:c_cords[1]]\n",
    "            k_depth_idx+= kernel_depth_step\n",
    "        \n",
    "        conv_filters = conv_filters_filtered\n",
    "        return conv_filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filters = torch.randn(size=(16, 16, 1, 1))\n",
    "div_factor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = ShardedKernels(4)\n",
    "x =sk(conv_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 4, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quad_coords(kernel_size):\n",
    "    step_size = max(1, kernel_size // 2)\n",
    "    cords_list = []\n",
    "    for r_start in range(0, kernel_size, step_size):\n",
    "        for c_start in range(0, kernel_size, step_size):\n",
    "            cords_list.append(((r_start, r_start + step_size), (c_start, c_start + step_size)))\n",
    "    return cords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 1), (0, 1))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_quad_coords(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_filters_filtered = torch.ones(size=(conv_filters.shape[0], conv_filters.shape[1] // div_factor, conv_filters.shape[2], conv_filters.shape[3]))\n",
    "quad_cords = generate_quad_coords(8)\n",
    "kernel_depth_step = conv_filters.shape[1] // div_factor\n",
    "k_depth_idx = 0\n",
    "for r_cords, c_cords in quad_cords:\n",
    "\n",
    "    conv_filters_filtered[:, :, r_cords[0]:r_cords[1], c_cords[0]:c_cords[1]] = conv_filters[:, k_depth_idx:k_depth_idx + kernel_depth_step, r_cords[0]:r_cords[1], c_cords[0]:c_cords[1]]\n",
    "    k_depth_idx+= kernel_depth_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1402e+00, -4.2928e-01,  2.8091e-01, -1.0190e+00, -5.9424e-01,\n",
       "          4.5974e-01,  2.6081e-01,  6.7590e-01],\n",
       "        [ 4.7654e-01, -5.4996e-01, -3.3806e+00, -8.3316e-01,  6.4199e-01,\n",
       "         -1.2206e+00, -2.3750e-01,  3.6996e-01],\n",
       "        [ 3.1071e-03,  1.3704e+00,  1.4806e+00, -2.0582e-01, -9.4039e-01,\n",
       "          7.5699e-01,  2.7783e-02, -8.2270e-01],\n",
       "        [-1.2132e+00,  6.1465e-01, -1.1997e+00, -8.8267e-01,  1.0682e+00,\n",
       "         -4.9999e-01, -5.7061e-01, -9.7227e-01],\n",
       "        [ 5.9675e-03,  5.3239e-01,  1.4067e+00, -1.3155e+00,  6.4387e-01,\n",
       "          6.1400e-01, -1.2567e-01,  6.3331e-01],\n",
       "        [-6.5907e-01,  1.6734e+00, -1.0897e-01,  1.0050e+00, -6.7881e-01,\n",
       "          1.3858e-01, -8.6043e-01, -7.4899e-01],\n",
       "        [ 1.7202e+00, -1.2103e+00, -8.6344e-01, -6.3969e-01, -1.0775e+00,\n",
       "          1.5935e+00,  1.2569e-01, -1.0253e-01],\n",
       "        [-1.8721e-01,  4.8032e-01,  1.6802e+00, -2.6866e-01, -8.1354e-01,\n",
       "         -1.1330e+00, -3.0445e-02, -5.7410e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_filters[0, 96, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9296,  0.3216,  1.4403,  0.3847, -2.1354, -0.2196,  0.4370, -2.1858],\n",
       "        [-0.2682, -0.9838, -1.0259,  0.8769,  0.6713, -0.6556, -0.0418,  0.5478],\n",
       "        [ 0.0592,  2.7598, -0.6658,  0.1691, -0.8939,  0.7662, -0.4438,  0.9105],\n",
       "        [ 0.3674,  0.0668, -1.1247,  0.9042, -1.4086,  0.5988,  0.1539, -0.1045],\n",
       "        [-2.0813, -0.7407, -0.4134,  0.8676,  0.6439,  0.6140, -0.1257,  0.6333],\n",
       "        [ 1.4898,  0.4647,  0.4583,  0.3006, -0.6788,  0.1386, -0.8604, -0.7490],\n",
       "        [-0.4112, -0.9134,  0.7305, -0.8308, -1.0775,  1.5935,  0.1257, -0.1025],\n",
       "        [ 0.3185, -0.9737,  1.4716, -0.4050, -0.8135, -1.1330, -0.0304, -0.5741]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_filters_filtered[0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((0, 4), (0, 4)), ((0, 4), (4, 8)), ((4, 8), (0, 4)), ((4, 8), (4, 8))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_factor = 2\n",
    "\n",
    "group_indices = torch.randint(size=(conv_filters.shape[0], conv_filters.shape[1] // div_factor, conv_filters.shape[2],\n",
    "                                             conv_filters.shape[3]), high = div_factor, low=0)\n",
    "offset_indices = torch.arange(start=0, end =conv_filters.shape[1], step=div_factor )\n",
    "offset_indices = offset_indices.reshape(-1, 1 ,1)\n",
    "indices = torch.add(group_indices, offset_indices).to(device=conv_filters.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 8, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 8, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_factor = 2\n",
    "# Gather first quad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_factor = 2\n",
    "group_indices = torch.randint(size=(conv_filters.shape[0], conv_filters.shape[1] // div_factor, conv_filters.shape[2],\n",
    "                                        conv_filters.shape[3]), high = div_factor, low=0)\n",
    "offset_indices = torch.arange(start=0, end =conv_filters.shape[1], step= div_factor)\n",
    "#offset_indices = offset_indices.reshape(-1, 1 ,1)\n",
    "#indices = torch.add(group_indices, offset_indices).to(device=conv_filters.device)\n",
    "#conv_filters = torch.gather(conv_filters, 1, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 8, 8])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_indices = torch.arange(start=0, end =conv_filters.shape[1], step= div_factor )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "32*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_dataset_path = \"/net/scratch/zsarwar/SparseDNNs/MT_cifar10_randCNN_10_8d64c1903fcb2ff6173fe22979c3175f/MT_Baseline_814e4c261078e7110bf94f724cd2187e/Adversarial_Datasets/CW_adv_samples_2500_test_detector-type-Regular_integrated-False_c-1.0_d-0.0_eps-0.034.pickle\"\n",
    "\n",
    "with open(adv_dataset_path, 'rb') as adv_set:\n",
    "    adv_samples = pickle.load(adv_set)\n",
    "\n",
    "\n",
    "\n",
    "transform_adv = transforms.Compose([\n",
    "    ])\n",
    "valset = CustomImageDataset_Adv(adv_samples, transform=transform_adv)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(valset, batch_size=8,\n",
    "                                            shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandResNetFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # Get a resnet18 backbone\n",
    "        m = model\n",
    "        return_nodes = {\n",
    "            # node_name: user-specified key for output dict\n",
    "            'x' : 'x',\n",
    "            'conv1': 'conv1',\n",
    "            'layer1.0.conv1': 'layer1.0.conv1',\n",
    "            'layer1.0.conv2': 'layer1.0.conv2',\n",
    "            'layer1.1.conv1': 'layer1.1.conv1',\n",
    "            'layer1.1.conv2': 'layer1.1.conv2',\n",
    "            'layer2.0.conv1': 'layer2.0.conv1',\n",
    "            'layer2.0.conv2': 'layer2.0.conv2',\n",
    "            'layer2.1.conv1': 'layer2.1.conv1',\n",
    "            'layer2.1.conv2': 'layer2.1.conv2',\n",
    "            'layer3.0.conv1': 'layer3.0.conv1',\n",
    "            'layer3.0.conv2': 'layer3.0.conv2',\n",
    "            'layer3.1.conv1': 'layer3.1.conv1',\n",
    "            'layer3.1.conv2': 'layer3.1.conv2',\n",
    "            'layer4.0.conv1': 'layer4.0.conv1',\n",
    "            'layer4.0.conv2': 'layer4.0.conv2',\n",
    "            'layer4.1.conv1': 'layer4.1.conv1',\n",
    "            'layer4.1.conv2': 'layer4.1.conv2',\n",
    "            'fc' : 'fc'\n",
    "      \n",
    "        }\n",
    "        self.body = create_feature_extractor(\n",
    "            m, return_nodes=return_nodes)\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.resnet_rand_tracer_1 import resnet18 as resnet18_1,  SparsifyKernelGroups as SparsifyKernelGroups_1 \n",
    "sparseblock_1 = SparsifyKernelGroups_1\n",
    "\n",
    "from utils.resnet_rand_tracer_2 import resnet18 as resnet18_2,  SparsifyKernelGroups as SparsifyKernelGroups_2\n",
    "sparseblock_2 = SparsifyKernelGroups_2\n",
    "scale_factor=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = resnet18_1(sparsefilter=sparseblock_1,scale_factor=scale_factor)                \n",
    "model_1.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "ckpt_path = \"/net/scratch/zsarwar/SparseDNNs/MT_cifar10_randCNN_10_8d64c1903fcb2ff6173fe22979c3175f/MT_Baseline_814e4c261078e7110bf94f724cd2187e/Checkpoints/model_best.pth.tar\"\n",
    "loc = 'cpu'\n",
    "checkpoint = torch.load(ckpt_path, map_location=loc)\n",
    "model_1.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model_1 = RandResNetFeatureExtractor(model_1)\n",
    "\n",
    "\n",
    "model_2 = resnet18_2(sparsefilter=sparseblock_2,scale_factor=scale_factor)                \n",
    "model_2.fc = nn.Linear(in_features=512, out_features=10, bias=True)\n",
    "\n",
    "ckpt_path = \"/net/scratch/zsarwar/SparseDNNs/MT_cifar10_randCNN_10_8d64c1903fcb2ff6173fe22979c3175f/MT_Baseline_814e4c261078e7110bf94f724cd2187e/Checkpoints/model_best.pth.tar\"\n",
    "loc = 'cpu'\n",
    "checkpoint = torch.load(ckpt_path, map_location=loc)\n",
    "model_2.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "model_2 = RandResNetFeatureExtractor(model_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(131)\n",
    "np.random.seed(131)\n",
    "\n",
    "torch.set_printoptions(precision=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in val_loader:\n",
    "    images = x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = model_1(images)\n",
    "pred_2 = model_2(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0244943742,  0.1756346375,  0.2234019637,  0.0828140825],\n",
       "        [-0.0387842506,  0.0129092969,  0.0315281153, -0.1097311974],\n",
       "        [ 0.0705896765, -0.0361112505, -0.0218053516, -0.0906050354],\n",
       "        [-0.1606753021, -0.2752042115, -0.1349949539, -0.0648586378]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_1['layer2.0.conv2'][0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0180419460,  0.3182912767,  0.1522731036,  0.0494774953],\n",
       "        [ 0.0247102417,  0.1215718761,  0.1180688888, -0.0719897375],\n",
       "        [ 0.1559844762,  0.0093886144,  0.0524684116, -0.0205788258],\n",
       "        [-0.1441754699, -0.3059452772, -0.1860212982, -0.1545179784]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_2['layer2.0.conv2'][0, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetFeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # Get a resnet18 backbone\n",
    "        m = model\n",
    "        return_nodes = {\n",
    "            # node_name: user-specified key for output dict\n",
    "            'layer1.0.conv1': 'layer1a.0.conv1',\n",
    "            'layer1.0.conv2': 'layer1.0.conv2',\n",
    "            'layer1.1.conv1': 'layer1.1.conv1',\n",
    "            'layer1.1.conv2': 'layer1.1.conv2',\n",
    "            'layer2.0.conv1': 'layer2.0.conv1',\n",
    "            'layer2.0.conv2': 'layer2.0.conv2',\n",
    "            'layer2.1.conv1': 'layer2.1.conv1',\n",
    "            'layer2.1.conv2': 'layer2.1.conv2',\n",
    "            'layer3.0.conv1': 'layer3.0.conv1',\n",
    "            'layer3.0.conv2': 'layer3.0.conv2',\n",
    "            'layer3.1.conv1': 'layer3.1.conv1',\n",
    "            'layer3.1.conv2': 'layer3.1.conv2',\n",
    "            'layer4.0.conv1': 'layer4.0.conv1',\n",
    "            'layer4.0.conv2': 'layer4.0.conv2',\n",
    "            'layer4.1.conv1': 'layer4.1.conv1',\n",
    "            'layer4.1.conv2': 'layer4.1.conv2'\n",
    "        }\n",
    "        self.body = create_feature_extractor(\n",
    "            m, return_nodes=return_nodes)\n",
    "    def forward(self, x):\n",
    "        x = self.body(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()\n",
    "model = ResNetFeatureExtractor(model)\n",
    "\n",
    "images = torch.randn(size=(8, 3, 32, 32))\n",
    "features = model(images)\n",
    "img = features['layer1.0.conv2'].detach().numpy()\n",
    "img = img[0]\n",
    "#img = img[:, :, 0]\n",
    "img = img.swapaxes(0,1)\n",
    "img = img.swapaxes(1,2)\n",
    "img = img[:, :, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_side_by_side(image, save_path=None, titles=None):\n",
    "    \"\"\"\n",
    "    Plots three images side by side.\n",
    "\n",
    "    Args:\n",
    "        image1 (np.ndarray): The first image.\n",
    "        image2 (np.ndarray): The second image.\n",
    "        image3 (np.ndarray): The third image.\n",
    "        titles (list, optional): A list of titles for the images.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    #axes[1].set_title(titles[1])\n",
    "    axes.imshow(image)\n",
    "    axes.axis('off')\n",
    "    \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_side_by_side(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from utils.resnet_rand import resnet18, SparsifyFiltersLayer, SparsifyKernelGroups\n",
    "sparseblock = SparsifyKernelGroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rand = resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/net/scratch/zsarwar/SparseDNNs/MT_cifar10_randCNN_10_8d64c1903fcb2ff6173fe22979c3175f/MT_Baseline_43528ed29912dd1a3825f30bea8e4ba8/\"\n",
    "\n",
    "\n",
    "stuff = \"Adversarial_Datasets/CW_adv_samples_512_test_detector-type-Regular_integrated-False_c-1.0_d-0.0_eps-0.002.pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_base_paths = [\"/net/scratch/zsarwar/SparseDNNs/MT_cifar10_randCNN_10_8d64c1903fcb2ff6173fe22979c3175f/MT_Baseline_43528ed29912dd1a3825f30bea8e4ba8/\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_num_samples = 512\n",
    "    \n",
    "for eps_idx, base_path in enumerate(all_base_paths):\n",
    "    detector='Regular'\n",
    "    num_samples = 512\n",
    "    c_base=1.0\n",
    "    d_base=0.0\n",
    "    all_flipped_indices = []\n",
    "    lst_theoretical_eps_per_channel = []\n",
    "    lst_mse_loss_per_channel = []\n",
    "    lst_mse_loss_per_pixel = []\n",
    "    lst_mse_max_per_pixel = []\n",
    "    lst_mse_loss_per_image = []\n",
    "\n",
    "    prev_flipped_indices = set()\n",
    "    eps = 0.2\n",
    "    images_cw_base = f\"Adversarial_Datasets/CW_adv_samples_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pickle\"\n",
    "    images_benign = f\"Benign_Datasets/CW_benign_samples_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pickle\"\n",
    "    predictions_base = f\"Predictions/Model/CW_type-adversarial_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pickle\"\n",
    "    filtered_indices_base = f\"Predictions/Perturbed_Samples/CW_benign_samples_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pt\"\n",
    "    images_cw_base = os.path.join(base_path, images_cw_base)\n",
    "    images_benign = os.path.join(base_path, images_benign)\n",
    "    predictions_base_path = os.path.join(base_path, predictions_base)\n",
    "    filtered_indices_base_path = os.path.join(base_path, filtered_indices_base)\n",
    "\n",
    "    # Filter by common indices\n",
    "    filtered_indices_base = torch.load(filtered_indices_base_path)\n",
    "    filtered_indices_base = [x.item() for x in filtered_indices_base]\n",
    "    curr_flipped_indices = set(filtered_indices_base)\n",
    "    curr_unique_flipped_indices = curr_flipped_indices.difference(prev_flipped_indices)    \n",
    "    prev_flipped_indices.update(curr_unique_flipped_indices)\n",
    "    curr_unique_flipped_indices = list(curr_unique_flipped_indices)\n",
    "    all_flipped_indices.append(filtered_indices_base)\n",
    "\n",
    "    with open(images_cw_base, 'rb') as in_file:\n",
    "        images_cw_base = pickle.load(in_file)\n",
    "        images_cw_base[0] = images_cw_base[0][curr_unique_flipped_indices]\n",
    "        images_cw_base[1] = images_cw_base[1][curr_unique_flipped_indices]\n",
    "        \n",
    "    with open(images_benign, 'rb') as in_file:\n",
    "        images_benign = pickle.load(in_file)\n",
    "        images_benign[0] = images_benign[0][curr_unique_flipped_indices]\n",
    "        images_benign[1] = images_benign[1][curr_unique_flipped_indices]\n",
    "\n",
    "    with open(predictions_base_path, 'rb') as in_file:\n",
    "        predictions = pickle.load(in_file)\n",
    "        true_labels_base = np.asarray(predictions['true_labels'])[curr_unique_flipped_indices]\n",
    "        pred_labels_base = np.asarray(predictions['pred_labels'])[curr_unique_flipped_indices]\n",
    "\n",
    "    #save_path = os.path.join(base_path, 'Visualizations')\n",
    "    \n",
    "\n",
    "    for idx in range(20):\n",
    "        benign = images_benign[0][idx]\n",
    "        cw_base = images_cw_base[0][idx]\n",
    "\n",
    "        benign = benign.swapaxes(0,1)\n",
    "        benign = benign.swapaxes(1,2)\n",
    "\n",
    "        cw_base = cw_base.swapaxes(0,1)\n",
    "        cw_base = cw_base.swapaxes(1,2)\n",
    "\n",
    "\n",
    "        title_benign = f'GT:{true_labels_base[idx]}'\n",
    "        title_base = f'Cls:{pred_labels_base[idx]}'\n",
    "        titles = [title_benign, title_base]\n",
    "        plot_side_by_side(benign, cw_base,  idx, titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_num_samples = 512\n",
    "    \n",
    "for eps_idx, base_path in enumerate(all_base_paths):\n",
    "    detector='Regular'\n",
    "    num_samples = 512\n",
    "    c_base=1.0\n",
    "    d_base=0.0\n",
    "    all_flipped_indices = []\n",
    "    lst_theoretical_eps_per_channel = []\n",
    "    lst_mse_loss_per_channel = []\n",
    "    lst_mse_loss_per_pixel = []\n",
    "    lst_mse_max_per_pixel = []\n",
    "    lst_mse_loss_per_image = []\n",
    "\n",
    "    prev_flipped_indices = set()\n",
    "    eps = 0.2\n",
    "    images_cw_base = f\"Adversarial_Datasets/CW_adv_samples_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pickle\"\n",
    "    images_benign = f\"Benign_Datasets/CW_benign_samples_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pickle\"\n",
    "    predictions_base = f\"Predictions/Model/CW_type-adversarial_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pickle\"\n",
    "    filtered_indices_base = f\"Predictions/Perturbed_Samples/CW_benign_samples_{num_samples}_test_detector-type-{detector}_integrated-False_c-{c_base}_d-{d_base}_eps-{eps}.pt\"\n",
    "    images_cw_base = os.path.join(base_path, images_cw_base)\n",
    "    images_benign = os.path.join(base_path, images_benign)\n",
    "    predictions_base_path = os.path.join(base_path, predictions_base)\n",
    "    filtered_indices_base_path = os.path.join(base_path, filtered_indices_base)\n",
    "\n",
    "\n",
    "    with open(images_benign, 'rb') as in_file:\n",
    "        images_benign = pickle.load(in_file)\n",
    "        images_benign[0] = images_benign[0][curr_unique_flipped_indices]\n",
    "        images_benign[1] = images_benign[1][curr_unique_flipped_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(size=(64, 3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gaussian = torch.randn_like(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3168)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_gaussian[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.randn(size=(10000,))\n",
    "mu, sigma = torch.tensor(10), torch.tensor(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0727, -0.8496,  0.8868,  ...,  0.4626, -0.4955,  0.0988])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22360679774997896"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pert_weights = torch.sqrt(sigma)*weights + mu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.9655)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_weights.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(20.0134)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pert_weights.std().square()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ffcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
